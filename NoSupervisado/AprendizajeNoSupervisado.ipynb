{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AprendizajeNoSupervisado.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNvLJtgxGYL8h6STJno6RC9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/garestrear/ninja-pythonist/blob/master/NoSupervisado/AprendizajeNoSupervisado.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S83vikzD3rbb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDIPHWOWjvjY"
      },
      "source": [
        "________________\r\n",
        "# <center> Reducción de dimensionalidad </center>\r\n",
        "________________\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dm3Sm1mImhHo"
      },
      "source": [
        "<div align='justify'>\r\n",
        "<h4>\r\n",
        "$\\circ$ Los algoritmos pertenecientes a esta familia tienen como objetivo reducir el número de características en una colección de datos de entrada, filtrando las características no tan relevantes y conservando la mayor cantidad posible de las interesantes. \r\n",
        "</h4>\r\n",
        "<h4>\r\n",
        "$\\circ$ Decimos que es una reducción de dimensionalidad porque proyectamos un conjunto de datos de entrada de gran dimensión (número original de características) en un conjunto de datos de menor dimensión (menor número de características).\r\n",
        "</h4>\r\n",
        "<h4>\r\n",
        "$\\circ$ La reducción de la dimensionalidad permite que el modelos no supervisado identifique patrones de manera más efectiva y resuelva de manera más eficiente problemas computacionalmente costosos a gran escala (que a menudo involucran imágenes, video, voz y texto).\r\n",
        "</h4>\r\n",
        "<h4>\r\n",
        "$\\circ$ Hay dos ramas principales de dimensionalidad: proyección lineal y reducción de dimensionalidad no lineal (manifold learning). Exploraremos dentro de la proyección lineal el <em> análisis de componentes principales (PCA) </em> y dentro de la reducción de dimensionalidad no lineal <em> t-SNE (t-distributed stochastic neighbor embedding )</em>. \r\n",
        "</h4>\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dil_ysppq7pD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaNnhaqTq8tK"
      },
      "source": [
        "________________\r\n",
        "# <center> Agrupación ─ Clustering </center>\r\n",
        "________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2S8As1Rq8-z"
      },
      "source": [
        "<div align='justify'>\r\n",
        "<h4>\r\n",
        "Una vez se han reducido el conjunto de características originales a un conjunto más pequeño y manejable, podemos encontrar patrones interesantes agrupando instancias similares de datos. Esto se conoce como agrupación (clustering) y se puede lograr con una variedad de algoritmos de aprendizaje no supervisados. \r\n",
        "</h4>\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-27vNbi9NG5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}