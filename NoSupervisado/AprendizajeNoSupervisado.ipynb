{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AprendizajeNoSupervisado.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNQ+VodAfqLMAqLVxEXPIGA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/garestrear/ninja-pythonist/blob/master/NoSupervisado/AprendizajeNoSupervisado.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S83vikzD3rbb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDIPHWOWjvjY"
      },
      "source": [
        "________________\r\n",
        "# <center> Reducción de dimensionalidad </center>\r\n",
        "________________\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dm3Sm1mImhHo"
      },
      "source": [
        "<div align='justify'>\r\n",
        "<h4>\r\n",
        "$\\circ$ Los algoritmos pertenecientes a esta familia tienen como objetivo reducir el número de características en una colección de datos de entrada, filtrando las características no tan relevantes y conservando la mayor cantidad posible de las interesantes. \r\n",
        "</h4>\r\n",
        "<h4>\r\n",
        "$\\circ$ Decimos que es una reducción de dimensionalidad porque proyectamos un conjunto de datos de entrada de gran dimensión (número original de características) en un conjunto de datos de menor dimensión (menor número de características).\r\n",
        "</h4>\r\n",
        "<h4>\r\n",
        "$\\circ$ La reducción de la dimensionalidad permite que el modelos no supervisado identifique patrones de manera más efectiva y resuelva de manera más eficiente problemas computacionalmente costosos a gran escala (que a menudo involucran imágenes, video, voz y texto).\r\n",
        "</h4>\r\n",
        "<h4>\r\n",
        "$\\circ$ Hay dos ramas principales de dimensionalidad: proyección lineal y reducción de dimensionalidad no lineal (manifold learning). Exploraremos dentro de la proyección lineal el <em> análisis de componentes principales (PCA) </em> y dentro de la reducción de dimensionalidad no lineal <em> t-SNE (t-distributed stochastic neighbor embedding )</em>. \r\n",
        "</h4>\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dil_ysppq7pD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaNnhaqTq8tK"
      },
      "source": [
        "________________\r\n",
        "# <center> Agrupamiento ─ Clustering </center>\r\n",
        "________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2S8As1Rq8-z"
      },
      "source": [
        "<div align='justify'>\r\n",
        "<h4>\r\n",
        "$\\circ$ Una vez se han reducido el conjunto de características originales a un conjunto más pequeño y manejable, podemos encontrar patrones interesantes agrupando instancias similares de datos. \r\n",
        "</h4>\r\n",
        "<h4>\r\n",
        "$\\circ$ El objetivo aquí es agrupar datos con base en su similitud: comparar cuán similares son los datos de una observación a los datos de otras observaciones. No se usan etiquetas.\r\n",
        "</h4>\r\n",
        "<h4>\r\n",
        "$\\circ$ Así un <strong> grupo (cluster) </strong> en este contexto hace referencia a un conjunto de datos con características similares. \r\n",
        "</h4>\r\n",
        "<h4>\r\n",
        "$\\circ$ El agrupamiento (clustering) se puede lograr con una variedad de algoritmos de aprendizaje no supervisados. Exploraremos los algoritmos <em> k-medias (k-means)</em> y <em>agrupamiento jerárquico (hierarchical clustering)</em> .\r\n",
        "</h4>\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xjd7Xs1FNJdc"
      },
      "source": [
        "## <center> k-medias ─ k-means </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLQXBBh_N_Ye"
      },
      "source": [
        "<div align='justify'>\r\n",
        "<h4>\r\n",
        "$\\circ$ En este algoritmo, especificamos de antemano el número de agrupaciones k deseadas. El algoritmo entonces asignará cada observación a exactamente una de estas k agrupaciones. \r\n",
        "</h4>\r\n",
        "<h4>\r\n",
        "$\\circ$ El agrupamiento por k-medias, primero asigna de manera aleatoria el centro de cada grupo. Luego, la distancia a estos centros es calculada para cada punto, se asigna el punto al cluster para cual la distancia al centro se más pequeña. Los centros de los grupos son cálculados nuevamente y esta iteración de asignar puntos a estos nuevos centros es realizada un número predefinido de veces.\r\n",
        "</h4>\r\n",
        "<h4>\r\n",
        "$\\circ$ Diferentes ejecuciones de k-medias darán como resultado asignaciones de grupos ligeramente diferentes pues esta inicialización aleatoria es una fuente de aleatoriedad, lo que resulta en asignaciones de agrupamiento ligeramente diferentes, de una ejecución de k-medias a otra. \r\n",
        "</h4>\r\n",
        "<h4>\r\n",
        "$\\circ$ El valor de k, el número de grupos, es un hiperparámetro que debe ser ajustado por el analista de datos. Existen algunas técnicas para seleccionar k. Ninguno de ellas ha demostrado ser óptima. La mayoría requiere que el analista haga una \"suposición fundamentada\" al observar algunas métricas o al examinar visualmente las asignaciones de grupos.\r\n",
        "</h4>\r\n",
        "<h4>\r\n",
        "$\\circ$ Un método para determinar el número de grupos consiste en dividir los datos en conjuntos de entrenamiento y pruebas, como en aprendizaje supervisado.\r\n",
        "</h4>\r\n",
        "<h4>\r\n",
        "$\\circ$ El algoritmo optimiza los grupos minimizando la variación dentro del grupo (también conocida como inercia) de modo que la suma de las variaciones dentro del grupo en todos los k grupos sea lo más pequeña posible.\r\n",
        "</h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-27vNbi9NG5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tS6Ad73ZNzOw"
      },
      "source": [
        "## <center> Agrupamiento jerárquico ─ Hierarchical clustering</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulKu7a7yOAIO"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trhoohpmN8jW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}